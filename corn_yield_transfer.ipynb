{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WVmEdcY3FUV"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWi9iDzhRlkj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, Dense, BatchNormalization, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import utils\n",
        "import numpy as np \n",
        "import h5py\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzR7OCkd8ivO"
      },
      "outputs": [],
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.utils.vis_utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q0OU-RdRwWo",
        "outputId": "bd8f2a24-28e5-4342-f26e-61e786366692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88vIbZEwRx1_",
        "outputId": "a95817c5-bf3d-4cea-a2b4-2f773352e28f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Oct 31 12:59:49 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpMWSYh42axo"
      },
      "source": [
        "# Dataset and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwhKA2U4qexH"
      },
      "outputs": [],
      "source": [
        "def average_results(avg):\n",
        "    avg_mape = [0, 0, 0, 0]\n",
        "    avg_rmse = [0, 0, 0, 0]\n",
        "    for i in range(len(avg)):\n",
        "        # print(avg)\n",
        "        for j in range(4):\n",
        "            avg_mape[j] += avg[i][j][0]\n",
        "            avg_rmse[j] += avg[i][j][2]\n",
        "    avg_mape = [x/len(avg) for x in avg_mape]\n",
        "    avg_rmse = [x/len(avg) for x in avg_rmse]\n",
        "    print(avg_mape)\n",
        "    print(avg_rmse)\n",
        "    return avg_mape, avg_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWEPzKu_vEqj"
      },
      "outputs": [],
      "source": [
        "def percentage_error(predictions, testing, median=False):\n",
        "    def calc_error(predicted, actual):\n",
        "        pe = abs((predicted - actual) / actual)\n",
        "        return pe\n",
        "    pe_scales = [i for i in range(0, 105, 5)]\n",
        "    # shows distruibution of errors\n",
        "    idk = [0] * 20\n",
        "    total = 0\n",
        "    median_pe = []\n",
        "    for i in range(len(predictions)):\n",
        "        pe = calc_error(float(predictions[i][0]), testing[i])\n",
        "        total += pe\n",
        "        for i in range(len(pe_scales) - 1):\n",
        "            if pe_scales[i] <= pe * 100 and pe * 100 <= pe_scales[i+1]:\n",
        "                idk[i] += 1\n",
        "        median_pe.append(pe*100)\n",
        "    ans = total / len(predictions)\n",
        "    if median:\n",
        "        median_pe.sort()\n",
        "        return ans, idk, median_pe[len(median_pe)//2]\n",
        "    return ans, idk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95cMhzmv2pe2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(models, X_test, y_test):\n",
        "    # individual\n",
        "    all_predictions = [None, None, None]\n",
        "\n",
        "    result = []\n",
        "\n",
        "\n",
        "    for i in range(3): \n",
        "        model_info = []\n",
        "        predictions = models[i].predict(X_test).tolist()\n",
        "        all_predictions[i] = predictions\n",
        "\n",
        "        mape, distrib = percentage_error(predictions, y_test)\n",
        "        temp = models[i].evaluate(X_test, y_test, verbose=0)\n",
        "        # print(temp)\n",
        "        loss, rmse = models[i].evaluate(X_test, y_test, verbose=2)\n",
        "        # print(loss, rmse)\n",
        "        model_info.append(mape)\n",
        "        model_info.append(distrib)\n",
        "        model_info.append(rmse)\n",
        "        result.append(model_info)\n",
        "    # ensemble\n",
        "    average_predictions = []\n",
        "    for i in range(len(y_test)):\n",
        "        temp1 = all_predictions[0][i][0]\n",
        "        temp2 = all_predictions[1][i][0]\n",
        "        temp3 = all_predictions[2][i][0]\n",
        "        \n",
        "        average_predictions.append([str((temp1 + temp2 + temp3) / 3)])\n",
        "    model_info = []\n",
        "    mape, distrib = percentage_error(average_predictions, y_test)\n",
        "    rmse = get_rmse(average_predictions, y_test)\n",
        "    model_info.append(mape)\n",
        "    model_info.append(distrib)\n",
        "    model_info.append(rmse)\n",
        "    result.append(model_info)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5UhkHaKzKLA"
      },
      "outputs": [],
      "source": [
        "def one_eval(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test).tolist()\n",
        "    mape, distrib = percentage_error(predictions, y_test)\n",
        "    loss, rmse = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print(mape, distrib, rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-7QeGYT2tab"
      },
      "outputs": [],
      "source": [
        "def avg_pe(model, dir):\n",
        "    region_dict = dict()\n",
        "    count = 0\n",
        "    for file in os.listdir(dir):\n",
        "        splitted = file.split('_')\n",
        "        output, name, year = float(splitted[0]), '_'.join(splitted[1:len(splitted)-1]), int(splitted[-1])\n",
        "        if year in [2016, 2017, 2018]:\n",
        "            pkl_file = open(dir + file, 'rb')\n",
        "            hist_3d = pickle.load(pkl_file)\n",
        "            hist_3d = hist_3d[:week_idx,:,:]\n",
        "                        \n",
        "            hist_3d = np.array([hist_3d])\n",
        "        \n",
        "            a = model.predict(hist_3d)\n",
        "\n",
        "            if name not in region_dict:\n",
        "                region_dict[name] = [None, None, None, -1]\n",
        "            region_dict[name][year-2016] = abs((model.predict([np.array(hist_3d)]).tolist()[0][0] - output) / output)\n",
        "            if None not in region_dict[name] and region_dict[name][-1] == -1:\n",
        "                region_dict[name][-1] = (region_dict[name][0] + region_dict[name][1] + region_dict[name][2]) / 3\n",
        "\n",
        "    return region_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQLCpSeAnoUW"
      },
      "outputs": [],
      "source": [
        "def normalize_hist(hist):\n",
        "    if float(hist.sum()) == 0:\n",
        "        return hist\n",
        "    hist = hist / float(hist.sum())\n",
        "    return hist    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybJ2qphQ2iev"
      },
      "outputs": [],
      "source": [
        "def get_rmse(prediction, actual):\n",
        "    count = 0\n",
        "    summ = 0\n",
        "    for i in range(len(prediction)):\n",
        "        # print(prediction[i], actual[i])\n",
        "        summ += (float(prediction[i][0]) - float(actual[i])) ** 2\n",
        "        count += 1\n",
        "    summ /= count\n",
        "    return summ ** 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxivRypneWrk"
      },
      "outputs": [],
      "source": [
        "def create_dataset(week_idx, main_dir, test_size, train_years, normalize=True):\n",
        "\n",
        "    # test dates = ['2015', '2016', '2017']\n",
        "    # remove from regular training data. Will evaluate later on. For example train 2008-2015, save 2016-2018 afterwards\n",
        "\n",
        "    # 32 x 9 x 26\n",
        "    \n",
        "\n",
        "    dir_len = os.scandir(main_dir)\n",
        "    # 23 19 15\n",
        "    # week idx changes weeks used. Like from week 1 (April) to week X\n",
        "    # week_idx = 26\n",
        "    X, y = [], []\n",
        "    X_2016, y_2016 = [], []\n",
        "    X_2017, y_2017 = [], []\n",
        "    X_2018, y_2018 = [], []\n",
        "\n",
        "    l = 0\n",
        "    count = 0\n",
        "    for file in os.listdir(main_dir):\n",
        "        output = float(file.split('_')[0])\n",
        "        pkl_file = open(main_dir + file, 'rb')\n",
        "        hist_3d = pickle.load(pkl_file)\n",
        "        # change a in :a for different month testing\n",
        "        hist_3d = hist_3d[:week_idx,:,:]\n",
        "        # print(hist_3d.shape)\n",
        "        if normalize:\n",
        "            for i in range(week_idx):\n",
        "                for j in range(9):\n",
        "                    temp = hist_3d[i,:,j]\n",
        "                    normal = normalize_hist(temp)\n",
        "                    hist_3d[i,:,j] = normal\n",
        "\n",
        "        # hist_3d = np.swapaxes(hist_3d,0,2)\n",
        "\n",
        "        # if '2016' in file or '2017' in file or '2018' in file:\n",
        "        #     X_2016.append(hist_3d)\n",
        "        #     y_2016.append(output)\n",
        "        # elif '2017' in file:\n",
        "        #     X_2017.append(hist_3d)\n",
        "        #     y_2017.append(output)\n",
        "        # elif '2018' in file:\n",
        "        #     X_2018.append(hist_3d)\n",
        "        #     y_2018.append(output)\n",
        "        # if '2018' in file or '2017' in file or '2016' in file or '2015' in file or '2014' in file  or '2013' in file:\n",
        "        #     l += 1\n",
        "        #     X.append(hist_3d)\n",
        "        #     y.append(output)\n",
        "\n",
        "        if '2016' in file:\n",
        "            X_2018.append(hist_3d)\n",
        "            y_2018.append(output)\n",
        "        elif '2017' in file: \n",
        "            X_2017.append(hist_3d)\n",
        "            y_2017.append(output)\n",
        "        # elif '2016' in file: \n",
        "        #     X_2016.append(hist_3d)\n",
        "        #     y_2016.append(output)\n",
        "\n",
        "        # if '2016' in file:\n",
        "        #     X_2016.append(hist_3d)\n",
        "        #     y_2016.append(output)\n",
        "        # elif '2017' in file:\n",
        "        #     X_2017.append(hist_3d)\n",
        "        #     y_2017.append(output)\n",
        "        # elif '2018' in file:\n",
        "        #     X_2018.append(hist_3d)\n",
        "        #     y_2018.append(output)\n",
        "        # else:\n",
        "        #     X.append(hist_3d)\n",
        "        #     y.append(output)\n",
        "\n",
        "\n",
        "        # lets just use one year of data\n",
        "        # if '2015' in file or '2014' in file or '2013' in file: \n",
        "        else:\n",
        "            for year in train_years:\n",
        "                if year in file:\n",
        "                    X.append(hist_3d)\n",
        "                    y.append(output)\n",
        "        count += 1 \n",
        "        # if len(X) > 175:\n",
        "        #     break\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    X, y = utils.shuffle(X, y)\n",
        "    X_2016, y_2016, X_2017, y_2017, X_2018, y_2018 = np.array(X_2016), np.array(y_2016), np.array(X_2017), np.array(y_2017), np.array(X_2018), np.array(y_2018)\n",
        "    if test_size == 0:\n",
        "        # We have no \"testing\" data\n",
        "        return X, None, y, None, X_2016, y_2016, X_2017, y_2017, X_2018, y_2018\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "    return X_train, X_test, y_train, y_test, X_2016, y_2016, X_2017, y_2017, X_2018, y_2018\n",
        "\n",
        "# # Regular US counties\n",
        "# us_dir = \"/content/drive/MyDrive/earth_hist_pkl2/\"\n",
        "# # China cities\n",
        "# china_dir = \"/content/drive/MyDrive/china_pkl2/\"\n",
        "# X_train, X_test, y_train, y_test = create_dataset(26, us_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LYfiJU91_5w"
      },
      "source": [
        "# 3d CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jZorNlhKxpd"
      },
      "outputs": [],
      "source": [
        "# Standard 3D CNN\n",
        "\n",
        "def cnn_3d(week_idx, save_dir, X_train, y_train, val_size=0.18):\n",
        "    # 9 32 26\n",
        "    sample_shape = (week_idx, 32, 9, 1)\n",
        "    model = Sequential()\n",
        "    model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 1), padding='Same'))\n",
        "    model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 1), padding='Same'))\n",
        "    model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 1), padding='Same'))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    checkpoint_filepath = '/content/drive/MyDrive/3d_cnn2'\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    earlystop_callback = EarlyStopping(monitor='val_loss', patience=30)\n",
        "    rmse = tf.keras.metrics.MeanSquaredError()\n",
        "    model.compile(loss=tf.keras.metrics.mean_squared_error,\n",
        "                  optimizer=Adam(lr=0.0005),\n",
        "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        "                  )\n",
        "    # model.summary()\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=16,\n",
        "                epochs=300,\n",
        "                verbose=1,\n",
        "                validation_split=val_size,\n",
        "                callbacks=[model_checkpoint_callback, earlystop_callback]\n",
        "                )\n",
        "    # predictions = model.predict(X_test).tolist()\n",
        "    # mape, distrib = percentage_error(predictions, y_test)\n",
        "    # print(mape)\n",
        "    # average_mape += mape\n",
        "    # test_loss = model.evaluate(X_test, y_test, verbose=2)\n",
        "    # Only save if training with US data\n",
        "    # model.save('/content/drive/MyDrive/earth_weight/3d_cnn.h5')\n",
        "    # model.save('/content/drive/MyDrive/earth_weight/3d_cnn_china.h5')\n",
        "    model.save(save_dir)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hsHPAwbAIH7"
      },
      "outputs": [],
      "source": [
        "# 3D-CNN transfer learning\n",
        "\n",
        "def cnn_3d_transfer(week_idx, X_train, y_train, val_size=0.18, keep_weights = False):\n",
        "    old_model = tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/3d_cnn.h5')\n",
        "    model = Sequential()\n",
        "    for layer in old_model.layers[:-5]: # this is where I changed your code\n",
        "        model.add(layer)    \n",
        "    # Freeze the layers \n",
        "    unfreeze = 999\n",
        "    count = 0\n",
        "    for layer in model.layers:\n",
        "        count += 1\n",
        "        if unfreeze <= count:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = keep_weights\n",
        "\n",
        "\n",
        "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    checkpoint_filepath = '/content/drive/MyDrive/3d_cnn_transfer2'\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    earlystop_callback = EarlyStopping(monitor='val_loss', patience=30)\n",
        "    rmse = tf.keras.metrics.MeanSquaredError()\n",
        "    model.compile(loss=tf.keras.metrics.mean_squared_error,\n",
        "                  optimizer=Adam(lr=0.001),\n",
        "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        "                  )\n",
        "    model.build((week_idx, 16, 9, 1))\n",
        "    # temp_model.summary()\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=16,\n",
        "                epochs=300,\n",
        "                verbose=0,\n",
        "                validation_split=val_size,\n",
        "                callbacks=[model_checkpoint_callback, earlystop_callback]\n",
        "                )\n",
        "    # predictions = model.predict(X_test).tolist()\n",
        "    # mape, distrib = percentage_error(predictions, y_test)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P3CgbMA2EmO"
      },
      "source": [
        "# CNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXsE5k74yM_n"
      },
      "outputs": [],
      "source": [
        "# CNN-LSTM\n",
        "\n",
        "def cnn_lstm(week_idx, save_dir, X_train, y_train, val_size=0.18):\n",
        "    \n",
        "    sample_shape = layers.Input((week_idx,32,9,1))\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size=(2,2), activation='relu', kernel_initializer='he_uniform', input_shape=(32,9,1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), padding='Same'))\n",
        "    model.add(Conv2D(128, kernel_size=(2,2), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), padding='Same'))\n",
        "    model.add(Conv2D(128, kernel_size=(2,2), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Flatten())\n",
        "    x = layers.TimeDistributed(model)(sample_shape)\n",
        "    x = layers.LSTM(256)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(128)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    out = layers.Dense(1)(x)\n",
        "\n",
        "    model = Model(sample_shape, out)\n",
        "    checkpoint_filepath = '/content/drive/MyDrive/cnn-lstm1'\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    earlystop_callback = EarlyStopping(monitor='val_loss', patience=30)\n",
        "    model.compile(loss=tf.keras.metrics.mean_squared_error, \n",
        "                  optimizer=Adam(lr=0.0005), \n",
        "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=16,\n",
        "                epochs=300,\n",
        "                verbose=0,\n",
        "                validation_split=val_size,\n",
        "                callbacks=[model_checkpoint_callback, earlystop_callback]\n",
        "                )\n",
        "    # model.save('/content/drive/MyDrive/earth_weight/cnn-lstm.h5')\n",
        "    # model.save('/content/drive/MyDrive/earth_weight/cnn-lstm_china.h5')\n",
        "    model.save(save_dir)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kopeWH6sGmcB"
      },
      "outputs": [],
      "source": [
        "# CNN-LSTM transfer learning\n",
        "\n",
        "def cnn_lstm_transfer(week_idx, X_train, y_train, val_size=0.18, keep_weights = False):\n",
        "    old_model = tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn-lstm.h5')\n",
        "    model = Sequential()\n",
        "    for layer in old_model.layers[:-5]: \n",
        "        model.add(layer) \n",
        "    # Freeze the layers \n",
        "    unfreeze = 999\n",
        "    count = 0\n",
        "    for layer in model.layers:\n",
        "        count += 1\n",
        "        if unfreeze <= count:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = keep_weights\n",
        "\n",
        "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    checkpoint_filepath = '/content/drive/MyDrive/cnn-lstm_transfer2'\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    rmse = tf.keras.metrics.MeanSquaredError()\n",
        "    model.compile(loss=tf.keras.metrics.mean_squared_error,\n",
        "                  optimizer=Adam(lr=0.0005),\n",
        "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        "                  )\n",
        "    # model.summary()\n",
        "    earlystop_callback = EarlyStopping(monitor='val_loss', patience=30)\n",
        "    # Fit data to model\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=16,\n",
        "                epochs=300,\n",
        "                verbose=0,\n",
        "                validation_split=val_size,\n",
        "                callbacks=[model_checkpoint_callback, earlystop_callback]\n",
        "                )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiiAwS1p2Ikc"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y83QpHzB3sOT"
      },
      "outputs": [],
      "source": [
        "# CNN\n",
        "\n",
        "def cnn(week_idx, save_dir, X_train, y_train, val_size=0.18):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size=(2,2), activation='relu', kernel_initializer='he_uniform', input_shape=(week_idx,32,9)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), padding='Same'))\n",
        "    model.add(Conv2D(128, kernel_size=(2,2), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), padding='Same'))\n",
        "    model.add(Conv2D(128, kernel_size=(2,2), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    checkpoint_filepath = '/content/drive/MyDrive/cnn2'\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    earlystop_callback = EarlyStopping(monitor='val_loss', patience=30)\n",
        "    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "    model.compile(loss=tf.keras.metrics.mean_squared_error,\n",
        "                  optimizer=Adam(lr=0.0005),\n",
        "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        "                  )\n",
        "    # model.summary()\n",
        "    # Fit data to model\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=16,\n",
        "                epochs=300,\n",
        "                verbose=0,\n",
        "                validation_split=val_size,\n",
        "                callbacks=[model_checkpoint_callback, earlystop_callback]\n",
        "                )\n",
        "    \n",
        "    # model.save('/content/drive/MyDrive/earth_weight/cnn.h5')\n",
        "    # model.save('/content/drive/MyDrive/earth_weight/cnn_china.h5')\n",
        "    model.save(save_dir)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l88nrGrVG0iK"
      },
      "outputs": [],
      "source": [
        "# CNN transfer learning\n",
        "\n",
        "def cnn_transfer(week_idx, X_train, y_train, val_size=0.18, keep_weights = False):\n",
        "    old_model = tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn.h5')\n",
        "    model = Sequential()\n",
        "    for layer in old_model.layers[:-5]: # this is where I changed your code\n",
        "        model.add(layer) \n",
        "    # Freeze the layers \n",
        "    unfreeze = 999\n",
        "    count = 0\n",
        "    for layer in model.layers:\n",
        "        count += 1\n",
        "        if unfreeze <= count:\n",
        "            print(layer)\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = keep_weights\n",
        "   \n",
        "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # for layer in old_model.layers[:]:\n",
        "    #     model.add(layer) \n",
        "\n",
        "    checkpoint_filepath = '/content/drive/MyDrive/cnn_transfer2'\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    rmse = tf.keras.metrics.MeanSquaredError()\n",
        "    model.compile(loss=tf.keras.metrics.mean_squared_error,\n",
        "                  optimizer=Adam(lr=0.005),\n",
        "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        "                  )\n",
        "    # model3.summary()\n",
        "    earlystop_callback = EarlyStopping(monitor='val_loss', patience=30)\n",
        "    # Fit data to model\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=16,\n",
        "                epochs=300,\n",
        "                verbose=0,\n",
        "                validation_split=val_size,\n",
        "                callbacks=[model_checkpoint_callback, earlystop_callback]\n",
        "                )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcwF4MUxwB2Q"
      },
      "source": [
        "# Multi Year Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjsNMspDtbty"
      },
      "outputs": [],
      "source": [
        "# Training everything\n",
        "# omg why this so long\n",
        "\n",
        "# US\n",
        "us_dir = \"/content/drive/MyDrive/earth_hist_pkl2/\"\n",
        "# China \n",
        "china_dir = \"/content/drive/MyDrive/china_new2pkl/\"\n",
        "\n",
        "all_china_reg_results = []\n",
        "all_china_transfer_results = []\n",
        "# 26 23 19 15\n",
        "week_idx = 15\n",
        "repeat = 1\n",
        "\n",
        "avg_us_2016, avg_us_2017, avg_us_2018 = [], [], []\n",
        "\n",
        "avg_china_reg_2016, avg_china_reg_2017, avg_china_reg_2018 = [], [], []\n",
        "avg_china_trans_2016, avg_china_trans_2017, avg_china_trans_2018 = [], [], []\n",
        "\n",
        "\n",
        "all_train_years = ['2015', '2014', '2013', '2012', '2011', '2010', '2009', '2008']\n",
        "# all_train_years = ['2015']\n",
        "\n",
        "# US models are trained on ALL YEARS \n",
        "X_train_us, X_test_us, y_train_us, y_test_us, X_us_2016, y_us_2016, X_us_2017, y_us_2017, X_us_2018, y_us_2018 = create_dataset(week_idx, us_dir, 0, all_train_years, normalize=False)\n",
        "# # Creating base models with US dataset\n",
        "print(\"3D cnn\")\n",
        "cnn_3d_us = cnn_3d(week_idx, '/content/drive/MyDrive/earth_weight/3d_cnn.h5', X_train_us, y_train_us, val_size=0.15)\n",
        "print(\"cnn lstm\")\n",
        "cnn_lstm_us = cnn_lstm(week_idx, '/content/drive/MyDrive/earth_weight/cnn-lstm.h5', X_train_us, y_train_us, val_size=0.15)\n",
        "print(\"cnn\")\n",
        "cnn_us = cnn(week_idx, '/content/drive/MyDrive/earth_weight/cnn.h5', X_train_us, y_train_us, val_size=0.15)\n",
        "\n",
        "print(\"US\")\n",
        "print(2016)\n",
        "us_eval_2016 = evaluate([cnn_3d_us, cnn_lstm_us, cnn_us], X_us_2016, y_us_2016)\n",
        "for data in us_eval_2016:\n",
        "    print(data)\n",
        "print(2017)\n",
        "us_eval_2017 = evaluate([cnn_3d_us, cnn_lstm_us, cnn_us], X_us_2017, y_us_2017)\n",
        "for data in us_eval_2016:\n",
        "    print(data)\n",
        "print(2018)\n",
        "us_eval_2018 = evaluate([cnn_3d_us, cnn_lstm_us, cnn_us], X_us_2018, y_us_2018)\n",
        "for data in us_eval_2018:\n",
        "    print(data)\n",
        "\n",
        "all_years_data = []\n",
        "for i in range(len(all_train_years)):  \n",
        "    train_years = all_train_years[:i+1]\n",
        "\n",
        "    avg_china_reg_2016, avg_china_reg_2017, avg_china_reg_2018 = [], [], []\n",
        "    avg_china_trans_2016, avg_china_trans_2017, avg_china_trans_2018 = [], [], []\n",
        "\n",
        "    for i in range(repeat):  \n",
        "\n",
        "        X_train_china, X_test_china, y_train_china, y_test_china, X_china_2016, y_china_2016, X_china_2017, y_china_2017, X_china_2018, y_china_2018 = create_dataset(week_idx, china_dir, 0, train_years, normalize=False)\n",
        "\n",
        "\n",
        "        print(train_years)\n",
        "\n",
        "        print(\"reg\")\n",
        "        print(\"cnn3d\")\n",
        "        cnn_3d_china = cnn_3d(week_idx, '/content/drive/MyDrive/earth_weight/3d_cnn_china.h5', X_train_china, y_train_china, val_size=0.2)\n",
        "        print(\"cnn lstm\")\n",
        "        cnn_lstm_china = cnn_lstm(week_idx, '/content/drive/MyDrive/earth_weight/cnn-lstm_china.h5', X_train_china, y_train_china, val_size=0.2)\n",
        "        print(\"cnn\")\n",
        "        cnn_china = cnn(week_idx, '/content/drive/MyDrive/earth_weight/cnn_china.h5', X_train_china, y_train_china, val_size=0.2)\n",
        "\n",
        "        print(2016)\n",
        "        china_eval_2016 = evaluate([cnn_3d_china, cnn_lstm_china, cnn_china], X_china_2016, y_china_2016)\n",
        "        for data in china_eval_2016:\n",
        "            print(data)\n",
        "        print(2017)\n",
        "        china_eval_2017 = evaluate([cnn_3d_china, cnn_lstm_china, cnn_china], X_china_2017, y_china_2017)\n",
        "        for data in china_eval_2016:\n",
        "            print(data)\n",
        "        print(2018)\n",
        "        china_eval_2018 = evaluate([cnn_3d_china, cnn_lstm_china, cnn_china], X_china_2018, y_china_2018)\n",
        "        for data in china_eval_2018:\n",
        "            print(data)\n",
        "\n",
        "        avg_china_reg_2016.append(china_eval_2016)\n",
        "        avg_china_reg_2017.append(china_eval_2017)\n",
        "        avg_china_reg_2018.append(china_eval_2018)\n",
        "\n",
        "        # cnn_3d_china = None\n",
        "        # cnn_lstm_china = None\n",
        "        # cnn_china = None\n",
        "        # china_eval_2016 = None\n",
        "        # china_eval_2017 = None\n",
        "        # china_eval_2018 = None\n",
        "\n",
        "        # Transfer learning with china dataset\n",
        "        print(\"trans\")\n",
        "        cnn_3d_trans = cnn_3d_transfer(week_idx, X_train_china, y_train_china, val_size=0.2)\n",
        "        cnn_lstm_trans = cnn_lstm_transfer(week_idx, X_train_china, y_train_china, val_size=0.2)\n",
        "        cnn_trans = cnn_transfer(week_idx, X_train_china, y_train_china, val_size=0.2)\n",
        "\n",
        "        print(\"trans\")\n",
        "        print(2016)\n",
        "        china_eval_2016 = evaluate([cnn_3d_trans, cnn_lstm_trans, cnn_trans], X_china_2016, y_china_2016)\n",
        "        for data in china_eval_2016:\n",
        "            print(data)\n",
        "        print(2017)\n",
        "        china_eval_2017 = evaluate([cnn_3d_trans, cnn_lstm_trans, cnn_trans], X_china_2017, y_china_2017)\n",
        "        for data in china_eval_2016:\n",
        "            print(data)\n",
        "        print(2018)\n",
        "        china_eval_2018 = evaluate([cnn_3d_trans, cnn_lstm_trans, cnn_trans], X_china_2018, y_china_2018)\n",
        "        for data in china_eval_2018:\n",
        "            print(data)\n",
        "            \n",
        "        avg_china_trans_2016.append(china_eval_2016)\n",
        "        avg_china_trans_2017.append(china_eval_2017)\n",
        "        avg_china_trans_2018.append(china_eval_2018)\n",
        "\n",
        "\n",
        "        # cnn_3d_trans = None\n",
        "        # cnn_lstm_trans = None\n",
        "        # cnn_trans = None\n",
        "        # china_eval_2016 = None\n",
        "        # china_eval_2017 = None\n",
        "        # china_eval_2018 = None\n",
        "    \n",
        "    all_years_data.append(train_years)\n",
        "    all_years_data.append(\"China reg\")\n",
        "    all_years_data.append(2016)\n",
        "    mape, rmse = average_results(avg_china_reg_2016)\n",
        "    all_years_data.append(mape)\n",
        "    all_years_data.append(rmse)\n",
        "    all_years_data.append(2017)\n",
        "    mape, rmse = average_results(avg_china_reg_2017)\n",
        "    all_years_data.append(mape)\n",
        "    all_years_data.append(rmse)\n",
        "    all_years_data.append(2018)\n",
        "    mape, rmse = average_results(avg_china_reg_2018)\n",
        "    all_years_data.append(mape)\n",
        "    all_years_data.append(rmse)\n",
        "\n",
        "    all_years_data.append(\"China trans\")\n",
        "    all_years_data.append(2016)\n",
        "    amape, rmse = average_results(avg_china_trans_2016)\n",
        "    all_years_data.append(mape)\n",
        "    all_years_data.append(rmse)\n",
        "    all_years_data.append(2017)\n",
        "    mape, rmse = average_results(avg_china_trans_2017)\n",
        "    all_years_data.append(mape)\n",
        "    all_years_data.append(rmse)\n",
        "    all_years_data.append(2018)\n",
        "    mape, rmse = average_results(avg_china_trans_2018)\n",
        "    all_years_data.append(mape)\n",
        "    all_years_data.append(rmse)\n",
        "\n",
        "    all_years_data.append(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixgrx8qE2L2K"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xiXKVwaiGalk"
      },
      "outputs": [],
      "source": [
        "# Training everything\n",
        "# omg why this so long\n",
        "\n",
        "# US\n",
        "us_dir = \"/content/drive/MyDrive/earth_hist_pkl2/\"\n",
        "# China \n",
        "china_dir = \"/content/drive/MyDrive/china_new2pkl/\"\n",
        "# Ukraine\n",
        "ukraine_dir = \"/content/drive/MyDrive/ukraine_pkl/\"\n",
        "\n",
        "all_china_reg_results = []\n",
        "all_china_transfer_results = []\n",
        "# 26 23 19 15\n",
        "week_idx = 15\n",
        "repeat = 3\n",
        "\n",
        "avg_us_2016, avg_us_2017, avg_us_2018 = [], [], []\n",
        "\n",
        "avg_china_reg_2016, avg_china_reg_2017, avg_china_reg_2018 = [], [], []\n",
        "avg_china_trans_2016, avg_china_trans_2017, avg_china_trans_2018 = [], [], []\n",
        "\n",
        "avg_ukraine, avg_trans_ukraine = [], []\n",
        "avg_ukraine_2017, avg_trans_ukraine_2017 = [], []\n",
        "\n",
        "all_train_year_data = []\n",
        "\n",
        "# train_years = ['2015', '2014', '2013', '2012', '2011', '2010', '2009', '2008']\n",
        "train_years = ['2018']\n",
        "\n",
        "# X_train_us, X_test_us, y_train_us, y_test_us, X_us_2016, y_us_2016, X_us_2017, y_us_2017, X_us_2018, y_us_2018 = create_dataset(week_idx, us_dir, 0, train_years, normalize=False)\n",
        "\n",
        "for i in range(repeat):  \n",
        " \n",
        "    # X_train_china, X_test_china, y_train_china, y_test_china, X_china_2016, y_china_2016, X_china_2017, y_china_2017, X_china_2018, y_china_2018 = create_dataset(week_idx, china_dir, 0, train_years, normalize=False)\n",
        "    X_train_ukraine, X_test_ukraine, y_train_ukraine, y_test_ukraine, X_ukraine_2016, y_ukraine_2016, X_ukraine_2017, y_ukraine_2017, X_ukraine_2018, y_ukraine_2018 = create_dataset(week_idx, ukraine_dir, 0, train_years, normalize=False)\n",
        "\n",
        "\n",
        "\n",
        "    # Creating base models with US dataset\n",
        "    # print(\"3D cnn\")\n",
        "    # cnn_3d_us = cnn_3d(week_idx, '/content/drive/MyDrive/earth_weight/3d_cnn.h5', X_train_us, y_train_us, val_size=0.15)\n",
        "    # print(\"cnn lstm\")\n",
        "    # cnn_lstm_us = cnn_lstm(week_idx, '/content/drive/MyDrive/earth_weight/cnn-lstm.h5', X_train_us, y_train_us, val_size=0.15)\n",
        "    # print(\"cnn\")\n",
        "    # cnn_us = cnn(week_idx, '/content/drive/MyDrive/earth_weight/cnn.h5', X_train_us, y_train_us, val_size=0.15)\n",
        "    # break\n",
        "\n",
        "    # print(\"US\")\n",
        "    # print(2016)\n",
        "    # us_eval_2016 = evaluate([cnn_3d_us, cnn_lstm_us, cnn_us], X_us_2016, y_us_2016)\n",
        "    # for data in us_eval_2016:\n",
        "    #     print(data)\n",
        "    # print(2017)\n",
        "    # us_eval_2017 = evaluate([cnn_3d_us, cnn_lstm_us, cnn_us], X_us_2017, y_us_2017)\n",
        "    # for data in us_eval_2016:\n",
        "    #     print(data)\n",
        "    # print(2018)\n",
        "    # us_eval_2018 = evaluate([cnn_3d_us, cnn_lstm_us, cnn_us], X_us_2018, y_us_2018)\n",
        "    # for data in us_eval_2018:\n",
        "    #     print(data)\n",
        "    # break\n",
        "\n",
        "    # print(avg_pe(cnn_3d_us, us_dir))\n",
        "    # print(avg_pe(cnn_lstm_us, us_dir))\n",
        "    # print(avg_pe(cnn_us, us_dir))\n",
        "\n",
        "    # avg_us_2016.append(us_eval_2016)\n",
        "    # avg_us_2017.append(us_eval_2017)\n",
        "    # avg_us_2018.append(us_eval_2018)\n",
        "\n",
        "    # cnn_3d_us = None\n",
        "    # cnn_lstm_us = None\n",
        "    # cnn_us = None\n",
        "    # us_eval_2016 = None\n",
        "    # us_eval_2017 = None\n",
        "    # us_eval_2018 = None\n",
        "    \n",
        "    # # US results\n",
        "    # print(\"US\")\n",
        "    # US_eval = evaluate([tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/3d_cnn.h5'),tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn-lstm.h5'), tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn.h5')], X_test_us, y_test_us)\n",
        "\n",
        "\n",
        "    # # print(2016)\n",
        "    # US_eval_2016 = evaluate([tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/3d_cnn.h5'),tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn-lstm.h5'), tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn.h5')], X_us_2016, y_us_2016)\n",
        "    # for data in US_eval_2016:\n",
        "    #     print(data)\n",
        "    # print(2017)\n",
        "    # US_eval_2017 = evaluate([tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/3d_cnn.h5'),tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn-lstm.h5'), tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn.h5')], X_us_2017, y_us_2017)\n",
        "    # for data in US_eval_2017:\n",
        "    #     print(data)\n",
        "    # print(2018)\n",
        "    # US_eval_2018 = evaluate([tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/3d_cnn.h5'),tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn-lstm.h5'), tf.keras.models.load_model('/content/drive/MyDrive/earth_weight/cnn.h5')], X_us_2018, y_us_2018)\n",
        "    # for data in US_eval_2018:\n",
        "    #     print(data)\n",
        "    # US_eval_2016, US_eval_2017, US_eval_2018 = None, None, None\n",
        "\n",
        "\n",
        "    # UKRAINE\n",
        "    print(\"reg\")\n",
        "    cnn_3d_ukraine = cnn_3d(week_idx, '/content/drive/MyDrive/earth_weight/3d_cnn_ukraine.h5', X_train_ukraine, y_train_ukraine, val_size=0.3)\n",
        "    print(\"cnn lstm\")\n",
        "    cnn_lstm_ukraine = cnn_lstm(week_idx, '/content/drive/MyDrive/earth_weight/cnn-lstm_ukraine.h5', X_train_ukraine, y_train_ukraine, val_size=0.3)\n",
        "    print(\"cnn\")\n",
        "    cnn_ukraine = cnn(week_idx, '/content/drive/MyDrive/earth_weight/cnn_ukraine.h5', X_train_ukraine, y_train_ukraine, val_size=0.3)\n",
        "\n",
        "    print(2018)\n",
        "    ukraine_eval_2018 = evaluate([cnn_3d_ukraine, cnn_lstm_ukraine, cnn_ukraine], X_ukraine_2018, y_ukraine_2018)\n",
        "    for data in ukraine_eval_2018:\n",
        "        print(data)\n",
        "\n",
        "    avg_ukraine.append(ukraine_eval_2018)\n",
        "\n",
        "    print(2017)\n",
        "    ukraine_eval_2017 = evaluate([cnn_3d_ukraine, cnn_lstm_ukraine, cnn_ukraine], X_ukraine_2017, y_ukraine_2017)\n",
        "    for data in ukraine_eval_2017:\n",
        "        print(data)\n",
        "\n",
        "    avg_ukraine_2017.append(ukraine_eval_2017)\n",
        "\n",
        "    \n",
        "    # Transfer learning \n",
        "    print(\"trans\")\n",
        "    cnn_3d_trans = cnn_3d_transfer(week_idx, X_train_ukraine, y_train_ukraine, val_size=0.3)\n",
        "    cnn_lstm_trans = cnn_lstm_transfer(week_idx, X_train_ukraine, y_train_ukraine, val_size=0.3)\n",
        "    cnn_trans = cnn_transfer(week_idx, X_train_ukraine, y_train_ukraine, val_size=0.3)\n",
        "    print(2018)\n",
        "    ukraine_trans_eval_2018 = evaluate([cnn_3d_trans, cnn_lstm_trans, cnn_trans], X_ukraine_2018, y_ukraine_2018)\n",
        "    for data in ukraine_trans_eval_2018:\n",
        "        print(data)\n",
        "\n",
        "    avg_trans_ukraine.append(ukraine_trans_eval_2018)\n",
        "\n",
        "    ukraine_trans_eval_2017 = evaluate([cnn_3d_trans, cnn_lstm_trans, cnn_trans], X_ukraine_2017, y_ukraine_2017)\n",
        "    for data in ukraine_trans_eval_2017:\n",
        "        print(data)\n",
        "\n",
        "    avg_trans_ukraine_2017.append(ukraine_trans_eval_2017)\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    # print(\"reg\")\n",
        "    # print(\"cnn3d\")\n",
        "    # cnn_3d_china = cnn_3d(week_idx, '/content/drive/MyDrive/earth_weight/3d_cnn_china.h5', X_train_china, y_train_china, val_size=0.2)\n",
        "    # print(\"cnn lstm\")\n",
        "    # cnn_lstm_china = cnn_lstm(week_idx, '/content/drive/MyDrive/earth_weight/cnn-lstm_china.h5', X_train_china, y_train_china, val_size=0.2)\n",
        "    # print(\"cnn\")\n",
        "    # cnn_china = cnn(week_idx, '/content/drive/MyDrive/earth_weight/cnn_china.h5', X_train_china, y_train_china, val_size=0.2)\n",
        "\n",
        "    # print(2016)\n",
        "    # china_eval_2016 = evaluate([cnn_3d_china, cnn_lstm_china, cnn_china], X_china_2016, y_china_2016)\n",
        "    # for data in china_eval_2016:\n",
        "    #     print(data)\n",
        "    # print(2017)\n",
        "    # china_eval_2017 = evaluate([cnn_3d_china, cnn_lstm_china, cnn_china], X_china_2017, y_china_2017)\n",
        "    # for data in china_eval_2016:\n",
        "    #     print(data)\n",
        "    # print(2018)\n",
        "    # china_eval_2018 = evaluate([cnn_3d_china, cnn_lstm_china, cnn_china], X_china_2018, y_china_2018)\n",
        "    # for data in china_eval_2018:\n",
        "    #     print(data)\n",
        "\n",
        "    # avg_china_reg_2016.append(china_eval_2016)\n",
        "    # avg_china_reg_2017.append(china_eval_2017)\n",
        "    # avg_china_reg_2018.append(china_eval_2018)\n",
        "\n",
        "    # cnn_3d_china = None\n",
        "    # cnn_lstm_china = None\n",
        "    # cnn_china = None\n",
        "    # china_eval_2016 = None\n",
        "    # china_eval_2017 = None\n",
        "    # china_eval_2018 = None\n",
        "\n",
        "\n",
        "    # # Transfer learning with china dataset\n",
        "    # print(\"trans\")\n",
        "    # cnn_3d_trans = cnn_3d_transfer(week_idx, X_train_china, y_train_china, val_size=0.2)\n",
        "    # cnn_lstm_trans = cnn_lstm_transfer(week_idx, X_train_china, y_train_china, val_size=0.2)\n",
        "    # cnn_trans = cnn_transfer(week_idx, X_train_china, y_train_china, val_size=0.2)\n",
        "\n",
        "    # print(\"trans\")\n",
        "    # print(2016)\n",
        "    # china_eval_2016 = evaluate([cnn_3d_trans, cnn_lstm_trans, cnn_trans], X_china_2016, y_china_2016)\n",
        "    # for data in china_eval_2016:\n",
        "    #     print(data)\n",
        "    # print(2017)\n",
        "    # china_eval_2017 = evaluate([cnn_3d_trans, cnn_lstm_trans, cnn_trans], X_china_2017, y_china_2017)\n",
        "    # for data in china_eval_2016:\n",
        "    #     print(data)\n",
        "    # print(2018)\n",
        "    # china_eval_2018 = evaluate([cnn_3d_trans, cnn_lstm_trans, cnn_trans], X_china_2018, y_china_2018)\n",
        "    # for data in china_eval_2018:\n",
        "    #     print(data)\n",
        "        \n",
        "    # avg_china_trans_2016.append(china_eval_2016)\n",
        "    # avg_china_trans_2017.append(china_eval_2017)\n",
        "    # avg_china_trans_2018.append(china_eval_2018)\n",
        "\n",
        "    # cnn_3d_trans = None\n",
        "    # cnn_lstm_trans = None\n",
        "    # cnn_trans = None\n",
        "    # china_eval_2016 = None\n",
        "    # china_eval_2017 = None\n",
        "    # china_eval_2018 = None\n",
        "\n",
        "     \n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMj4lsXIxvMfI7tY/tbjd+c"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}